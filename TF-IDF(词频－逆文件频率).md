# TF-IDF(词频－逆文件频率)

## (Term Frequency-Inverse Document Frequency)

### 是一种统计方法，用来评估一字词对于一个文件集或者一个语料库中的其中一份文件的重要程度，*字词的重要性随着他在文件中出现的次数成正比增长，但同时会随着他在语料库中出现的频率成反比下降。*

*词频（term frequency TF）* 指某一个给定的词语在在文件中出现的次数，这个数字通常会被归一化（一般是词频除以文章词总数），以防止它偏向长的文件。（同一个词与长文件中可能会比短文件有更高的词频，而不管该词语重要与否。）

但是, 需要注意, 一些通用的词语对于主题并没有太大的作用, 反倒是一些出现频率较少的词才能够表达文章的主题, 所以单纯使用是TF不合适的。权重的设计必须满足：**一个词预测主题的能力越强，权重越大，反之，权重越小**。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章的主题的作用很大，这些词的权重应该设计的较大。IDF就是在完成这样的工作.

公式:

TFw=在某一类中词条w出现的次数/该类中所有的词条数目

*逆向文件频率(Inverse document frequency IDF)* IDF的主要思想是：如果包含此条ｔ的文档越少，IDF越大，则说明词条具有很好的类别区分能力，某一特定词语IDF,可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。

$$IDF=log(\frac{语料库的文档总数}{包含词条w的文档数+1}),分母之所以要加１，是为了避免分母为０​$$

*TF-IDF倾向于过滤掉常见的词语，保留重要的词语。*

$$TF-IDF=TF*IDF$$



​       TF-IDF（termfrequency–inverse document frequency）是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随著它在文件中出现的次数成正比增加，但同时会随著它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜寻引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，因特网上的搜寻引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。

 

​      在文本挖掘中，要对文本库分词，而分词后需要对个每个分词计算它的权重，而这个权重可以使用TF-IDF计算。

​      TF(term frequency)就是分词出现的频率：该分词在该文档中出现的频率，算法是：（该分词在该文档出现的次数）/(该文档分词的总数)，这个值越大表示这个词越重要，即权重就越大。

例如：一篇文档分词后，总共有500个分词，而分词”Hello”出现的次数是20次，则TF值是： tf =20/500=2/50=0.04 

 

​       IDF（inversedocument frequency）逆向文件频率,一个文档库中，一个分词出现在的文档数越少越能和其它文档区别开来。算法是： log((总文档数/出现该分词的文档数)+0.01) ；（注加上0.01是为了防止log计算返回值为0）。 

例如：一个文档库中总共有50篇文档，2篇文档中出现过“Hello”分词，则idf是： 

Idf = log(50/2 + 0.01) = log(25.01)=1.39811369 

 

​       TF-IDF结合计算就是 tf*idf,比如上面的“Hello”分词例子中：

​        TF-IDF = tf* idf = (20/500)* log(50/2 + 0.01)= 0.04*1.39811369=0.0559245476